{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          X         Y         Z     Mixed  ClassLabel\n",
      "0  0.125022  0.094986  0.001297  0.157018         1.0\n",
      "1  0.150710  0.083282 -0.023514  0.173788         1.0\n",
      "2  0.102941  0.111084  0.010075  0.151782         1.0\n",
      "3  0.038450  0.049911  0.007511  0.063451         1.0\n",
      "4 -0.029148 -0.105423  0.017124  0.110711         1.0\n",
      "Index(['X', 'Y', 'Z', 'Mixed', 'ClassLabel'], dtype='object')\n",
      "X             float64\n",
      "Y             float64\n",
      "Z             float64\n",
      "Mixed         float64\n",
      "ClassLabel    float64\n",
      "dtype: object\n",
      "Naive Bayes Cross-validation scores: [0.57142857 0.51020408 0.59183673 0.63265306 0.55102041 0.69387755\n",
      " 0.67346939 0.66666667 0.6875     0.625     ]\n",
      "Naive Bayes Mean cross-validation score: 0.6203656462585034\n",
      "Naive Bayes Accuracy: 0.5510204081632653\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.64      0.47      0.54        15\n",
      "         2.0       1.00      1.00      1.00         4\n",
      "         3.0       0.45      0.62      0.53         8\n",
      "         4.0       0.46      1.00      0.63         6\n",
      "         5.0       0.50      0.31      0.38        16\n",
      "\n",
      "    accuracy                           0.55        49\n",
      "   macro avg       0.61      0.68      0.62        49\n",
      "weighted avg       0.57      0.55      0.54        49\n",
      "\n",
      "Random Forest Cross-validation scores: [0.6122449  0.57142857 0.57142857 0.67346939 0.53061224 0.73469388\n",
      " 0.65306122 0.72916667 0.64583333 0.54166667]\n",
      "Random Forest Mean cross-validation score: 0.6263605442176872\n",
      "Random Forest Accuracy: 0.5306122448979592\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.70      0.47      0.56        15\n",
      "         2.0       1.00      1.00      1.00         4\n",
      "         3.0       0.33      0.50      0.40         8\n",
      "         4.0       0.36      0.83      0.50         6\n",
      "         5.0       0.67      0.38      0.48        16\n",
      "\n",
      "    accuracy                           0.53        49\n",
      "   macro avg       0.61      0.64      0.59        49\n",
      "weighted avg       0.61      0.53      0.54        49\n",
      "\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "120 fits failed out of a total of 360.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "42 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.6438349  0.64602926 0.65514629\n",
      " 0.6438349  0.64602926 0.65514629        nan        nan        nan\n",
      " 0.64375653 0.65747126 0.66196447 0.64375653 0.65747126 0.66196447\n",
      "        nan        nan        nan 0.65752351 0.66664054 0.6460815\n",
      " 0.65752351 0.66664054 0.6460815         nan        nan        nan\n",
      " 0.63704284 0.65065308 0.65747126 0.63704284 0.65065308 0.65747126\n",
      "        nan        nan        nan 0.62776907 0.6368861  0.6346395\n",
      " 0.62776907 0.6368861  0.6346395         nan        nan        nan\n",
      " 0.62549634 0.65287356 0.65514629 0.62549634 0.65287356 0.65514629\n",
      "        nan        nan        nan 0.64143156 0.64819749 0.64602926\n",
      " 0.64143156 0.64819749 0.64602926        nan        nan        nan\n",
      " 0.62327586 0.63466562 0.64370428 0.62327586 0.63466562 0.64370428]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Accuracy after Hyperparameter Tuning: 0.4489795918367347\n",
      "Best Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.50      0.33      0.40        15\n",
      "         2.0       1.00      1.00      1.00         4\n",
      "         3.0       0.31      0.62      0.42         8\n",
      "         4.0       0.36      0.83      0.50         6\n",
      "         5.0       0.60      0.19      0.29        16\n",
      "\n",
      "    accuracy                           0.45        49\n",
      "   macro avg       0.55      0.60      0.52        49\n",
      "weighted avg       0.53      0.45      0.43        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Dataset/dataset_no_outlier.csv')\n",
    "\n",
    "# Print first few rows and column names to understand the format\n",
    "print(data.head())\n",
    "print(data.columns)\n",
    "\n",
    "# Assuming the class label column is named differently, let's identify it\n",
    "class_column = 'ClassLabel'  # Replace with the actual column name if different\n",
    "\n",
    "# If necessary, rename the class column to 'ClassLabel'\n",
    "data.rename(columns={class_column: 'ClassLabel'}, inplace=True)\n",
    "\n",
    "# Check the data types of the columns\n",
    "print(data.dtypes)\n",
    "\n",
    "# Aggregate every 10 rows into a sequence\n",
    "def aggregate_data(df, window_size=10):\n",
    "    aggregated_data = {'X': [], 'Y': [], 'Z': [], 'ClassLabel': []}\n",
    "    \n",
    "    for i in range(0, len(df), window_size):\n",
    "        if i + window_size <= len(df):\n",
    "            window = df.iloc[i:i+window_size]\n",
    "            aggregated_data['X'].append(window['X'].values)\n",
    "            aggregated_data['Y'].append(window['Y'].values)\n",
    "            aggregated_data['Z'].append(window['Z'].values)\n",
    "            # Assuming all rows in the window belong to the same class\n",
    "            aggregated_data['ClassLabel'].append(window['ClassLabel'].mode()[0])\n",
    "    \n",
    "    return pd.DataFrame(aggregated_data)\n",
    "\n",
    "# Aggregate data\n",
    "agg_data = aggregate_data(data, window_size=10)\n",
    "\n",
    "# Function to apply FFT and extract features\n",
    "def extract_fft_features(row):\n",
    "    # Apply FFT to each of the signal columns\n",
    "    fft_x = np.abs(fft(row['X']))\n",
    "    fft_y = np.abs(fft(row['Y']))\n",
    "    fft_z = np.abs(fft(row['Z']))\n",
    "    \n",
    "    # Take the first few coefficients as features\n",
    "    features = np.concatenate([fft_x[:5], fft_y[:5], fft_z[:5]])\n",
    "    return features\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "fft_features = agg_data.apply(extract_fft_features, axis=1)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "fft_features_df = pd.DataFrame(fft_features.tolist())\n",
    "\n",
    "# Combine FFT features with the class label\n",
    "fft_features_df['ClassLabel'] = agg_data['ClassLabel']\n",
    "\n",
    "# Export FFT features to CSV\n",
    "fft_features_df.to_csv('fft_features.csv', index=False)\n",
    "\n",
    "# Features and labels\n",
    "X = fft_features_df.drop(columns=['ClassLabel'])\n",
    "y = fft_features_df['ClassLabel']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_clf = GaussianNB()\n",
    "\n",
    "# Apply cross-validation\n",
    "nb_cv_scores = cross_val_score(nb_clf, X, y, cv=10)  # 10-fold cross-validation\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(f'Naive Bayes Cross-validation scores: {nb_cv_scores}')\n",
    "print(f'Naive Bayes Mean cross-validation score: {nb_cv_scores.mean()}')\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "nb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "nb_y_pred = nb_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "nb_accuracy = accuracy_score(y_test, nb_y_pred)\n",
    "print(f'Naive Bayes Accuracy: {nb_accuracy}')\n",
    "\n",
    "# Print classification report\n",
    "print('Naive Bayes Classification Report:')\n",
    "print(classification_report(y_test, nb_y_pred))\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Apply cross-validation\n",
    "rf_cv_scores = cross_val_score(rf_clf, X, y, cv=10)  # 10-fold cross-validation\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(f'Random Forest Cross-validation scores: {rf_cv_scores}')\n",
    "print(f'Random Forest Mean cross-validation score: {rf_cv_scores.mean()}')\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "print(f'Random Forest Accuracy: {rf_accuracy}')\n",
    "\n",
    "# Print classification report\n",
    "print('Random Forest Classification Report:')\n",
    "print(classification_report(y_test, rf_y_pred))\n",
    "\n",
    "# Hyperparameter tuning for Random Forest (Optional)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [4, 6, 8, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf_clf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best estimator\n",
    "best_rf_y_pred = best_rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "best_rf_accuracy = accuracy_score(y_test, best_rf_y_pred)\n",
    "print(f'Best Random Forest Accuracy after Hyperparameter Tuning: {best_rf_accuracy}')\n",
    "\n",
    "# Print classification report\n",
    "print('Best Random Forest Classification Report:')\n",
    "print(classification_report(y_test, best_rf_y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
