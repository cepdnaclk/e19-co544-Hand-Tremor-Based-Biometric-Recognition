{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 200, Loss: nan\n",
      "Epoch 300, Loss: nan\n",
      "Epoch 400, Loss: nan\n",
      "Epoch 500, Loss: nan\n",
      "Epoch 600, Loss: nan\n",
      "Epoch 700, Loss: nan\n",
      "Epoch 800, Loss: nan\n",
      "Epoch 900, Loss: nan\n",
      "Fold accuracy: 0.2255125284738041\n",
      "Epoch 0, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 200, Loss: nan\n",
      "Epoch 300, Loss: nan\n",
      "Epoch 400, Loss: nan\n",
      "Epoch 500, Loss: nan\n",
      "Epoch 600, Loss: nan\n",
      "Epoch 700, Loss: nan\n",
      "Epoch 800, Loss: nan\n",
      "Epoch 900, Loss: nan\n",
      "Fold accuracy: 0.2255125284738041\n",
      "Epoch 0, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 200, Loss: nan\n",
      "Epoch 300, Loss: nan\n",
      "Epoch 400, Loss: nan\n",
      "Epoch 500, Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 114\u001b[0m\n\u001b[0;32m    112\u001b[0m db2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    113\u001b[0m dZ1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dZ2, W2\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m*\u001b[39m relu_derivative(H1)\n\u001b[1;32m--> 114\u001b[0m dW1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdZ1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m db1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Update weights and biases\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data = pd.read_csv('combined_metrics.csv')\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "# Get the shape of the data\n",
    "(N, P) = data.shape\n",
    "\n",
    "# Split data into input features X and true output Y\n",
    "X = data.values[:, :-1]  # All columns except the last one\n",
    "Y = data.values[:, -1]   # The last column\n",
    "\n",
    "if np.any(np.isinf(X)):\n",
    "    print(\"Input contains infinite values. Replacing with nan.\")\n",
    "    max_float64 = np.finfo(np.float64).max\n",
    "    X[np.isinf(X)] = max_float64\n",
    "\n",
    "# One-hot encode the output labels\n",
    "Y = np.array(\n",
    "    [[1, 0, 0, 0, 0] if (t == 1) else \n",
    "     [0, 1, 0, 0, 0] if (t == 2) else \n",
    "     [0, 0, 1, 0, 0] if (t == 3) else \n",
    "     [0, 0, 0, 1, 0] if (t == 4) else \n",
    "     [0, 0, 0, 0, 1] for t in Y]\n",
    ")\n",
    "\n",
    "# Different architectures\n",
    "hidden_layer_sizes = [\n",
    "    (7, 9, 7),        # Architecture 1\n",
    "    (8, 16, 8),       # Architecture 2\n",
    "    (16, 32, 16),     # Architecture 3\n",
    "    (32, 64, 32),     # Architecture 4\n",
    "    (64, 128, 64)     # Architecture 5\n",
    "]\n",
    "\n",
    "# Parameters for neural network\n",
    "input_neurons = X.shape[1]\n",
    "output_neurons = 5\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "\n",
    "# Store results for plotting\n",
    "results = []\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Define sigmoid and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define softmax\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Subtracting max value for numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "for hidden_neurons_1, hidden_neurons_2, hidden_neurons_3 in hidden_layer_sizes:\n",
    "    accuracies = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split data into training and test sets for this fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # Normalize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Initialize random weights and biases\n",
    "        np.random.seed(42)\n",
    "        W1 = np.random.rand(input_neurons, hidden_neurons_1)\n",
    "        b1 = np.random.rand(1, hidden_neurons_1)\n",
    "        W2 = np.random.rand(hidden_neurons_1, hidden_neurons_2)\n",
    "        b2 = np.random.rand(1, hidden_neurons_2)\n",
    "        W3 = np.random.rand(hidden_neurons_2, hidden_neurons_3)\n",
    "        b3 = np.random.rand(1, hidden_neurons_3)\n",
    "        W4 = np.random.rand(hidden_neurons_3, output_neurons)\n",
    "        b4 = np.random.rand(1, output_neurons)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            Z1 = np.dot(X_train, W1) + b1\n",
    "            H1 = relu(Z1)\n",
    "            Z2 = np.dot(H1, W2) + b2\n",
    "            H2 = relu(Z2)\n",
    "            Z3 = np.dot(H2, W3) + b3\n",
    "            H3 = relu(Z3)\n",
    "            Z4 = np.dot(H3, W4) + b4\n",
    "            Y_pred = relu(Z4)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = np.mean(np.square(Y_pred - Y_train))\n",
    "\n",
    "            # Backpropagation\n",
    "            dZ4 = (Y_pred - Y_train) * relu_derivative(Y_pred)\n",
    "            dW4 = np.dot(H3.T, dZ4)\n",
    "            db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
    "            dZ3 = np.dot(dZ4, W4.T) * relu_derivative(H3)\n",
    "            dW3 = np.dot(H2.T, dZ3)\n",
    "            db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
    "            dZ2 = np.dot(dZ3, W3.T) * relu_derivative(H2)\n",
    "            dW2 = np.dot(H1.T, dZ2)\n",
    "            db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "            dZ1 = np.dot(dZ2, W2.T) * relu_derivative(H1)\n",
    "            dW1 = np.dot(X_train.T, dZ1)\n",
    "            db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "            # Update weights and biases\n",
    "            W4 -= learning_rate * dW4\n",
    "            b4 -= learning_rate * db4\n",
    "            W3 -= learning_rate * dW3\n",
    "            b3 -= learning_rate * db3\n",
    "            W2 -= learning_rate * dW2\n",
    "            b2 -= learning_rate * db2\n",
    "            W1 -= learning_rate * dW1\n",
    "            b1 -= learning_rate * db1\n",
    "\n",
    "            # Print the loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "        # Evaluate the model on the test set for this fold\n",
    "        Z1 = np.dot(X_test, W1) + b1\n",
    "        H1 = relu(Z1)\n",
    "        Z2 = np.dot(H1, W2) + b2\n",
    "        H2 = relu(Z2)\n",
    "        Z3 = np.dot(H2, W3) + b3\n",
    "        H3 = relu(Z3)\n",
    "        Z4 = np.dot(H3, W4) + b4\n",
    "        Y_pred = relu(Z4)\n",
    "        Y_pred = softmax(Y_pred)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        arr_x = np.argmax(Y_pred, axis=1)\n",
    "        arr_y = np.argmax(Y_test, axis=1)\n",
    "        correct = np.sum(arr_x == arr_y)\n",
    "        accuracy = correct / len(arr_x)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f'Fold accuracy: {accuracy}')\n",
    "\n",
    "    # Calculate the average accuracy across all folds\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    print(f'Average accuracy for hidden layers {hidden_neurons_1}, {hidden_neurons_2}, and {hidden_neurons_3}: {average_accuracy}')\n",
    "    results.append((hidden_neurons_1, hidden_neurons_2, hidden_neurons_3, average_accuracy))\n",
    "\n",
    "# Plotting the results\n",
    "hidden_layer_descriptions = [f'{h1}-{h2}-{h3}' for h1, h2, h3 in hidden_layer_sizes]\n",
    "accuracies = [result[3] for result in results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(hidden_layer_descriptions, accuracies, color='skyblue')\n",
    "plt.xlabel('Hidden Layer Sizes')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.title('10-Fold Cross-Validation Accuracy for Different Hidden Layer Sizes')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
